{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to MiniFlow\n",
    "\n",
    "This is a very basic version of the deep learning library Tensorflow. I did this exercise as part of the Udacity Deep Learning Nanodegree.\n",
    "\n",
    "I am writing it up here to cement my understanding as well as to provide some value to whoever might read this. I therefore try to provide some explanation as to what's going on so that you might get a better (simple) understanding of what's going under the hood of tensorflow. My explanations are not super in depth as I assume prior knowledge of how Neural Networks work so the explanations act as more of a refresher. Even if you don't have that knowledge though, you may still get some value from this :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural networks, each node stores a value and points to other nodes in the network. So the first thing we need is a node class which will these nodes.\n",
    "\n",
    "#### The Node class has the following properties:\n",
    "- inbound_nodes - a list of all the nodes that to this node\n",
    "- outbound_nodes - a list of all the nodes to which this node points\n",
    "- value - the value stored in this node\n",
    "- gradients - a dictionary containing the gradient of the total error with respect to each inbound node. This will be used in backpropogation and gradient descent\n",
    "\n",
    "#### And the following methods:\n",
    "- forward - calculates the value stored at the node based on the inbound nodes during forward propogation\n",
    "- backward - calculates self.gradients during backpropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Node (object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "\n",
    "        #inputs to this node\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "\n",
    "        #outputs of this node\n",
    "        self.outbound_nodes  = []\n",
    "\n",
    "        #for each input node, add this node as an outbound node\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "        \n",
    "        #value of node\n",
    "        self.value = None\n",
    "        \n",
    "        #gradients used for gradient descent\n",
    "        self.gradients = {}\n",
    "\n",
    "    def forward(self):\n",
    "        '''\n",
    "        Forward Propogation\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self):\n",
    "        '''\n",
    "        Backpropogation\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tensorflow (and Miniflow), neural networks can be abstracted as a Directed Acyclic Graph of nodes which represent values and operations. \n",
    "- It is a graph because each node may be connected to many different other nodes. \n",
    "- It is directed because there is a direction in which the values are propogated. Each node combines values from its inbound nodes and then passes a new value on to its outbound nodes. \n",
    "- It is acyclic because there are no cycles, meaning there is no path from any given node back to itself. The values are propogated from inputs to outputs. \n",
    "\n",
    "---\n",
    "Since this graph is directed, we have to make sure that the value for a parent node is calculated before any of it's children. We must therefore perform a Topological Sort, which creates a list from the graph in which all any given node is placed after it's parents in the list. The Algorithm we implement here is Kahn's Algorithm.\n",
    "\n",
    "The feed_dict parameter is a dictionary in which the keys are Input nodes and the values are the values to be assigned to the respective nodes. Later we will see that input nodes need to have their values passed in to the forward function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(feed_dict):\n",
    "    '''\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm\n",
    "    \n",
    "    '''\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    #create graph G\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes] \n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {\"in\":set(), \"out\":set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            nodes.append(m)\n",
    "            if m not in G:\n",
    "                G[m] = {\"in\":set(), \"out\":set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "\n",
    "    S = set(input_nodes) #nodes with no incoming edge\n",
    "    L = [] # sorted list of nodes\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        #give inputs their values\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        #add node to the list since it has no remaining inbound nodes\n",
    "        L.append(n)\n",
    "\n",
    "        #look at all children of n... for each, if n is it's only parent, remove the link and add it to S\n",
    "        while len(G[n]['out']) > 0:\n",
    "            m = G[n]['out'].pop()\n",
    "            G[m]['in'].remove(n)\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m) \n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have a valid order in which we are able to execute each node, we can now call forward pass, which goes through the sorted list of nodes and calculates the values for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(output_node, sorted_nodes):\n",
    "    '''\n",
    "    Performs a forward pass through the list of sorted nodes\n",
    "    '''\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "    return output_node.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes we will actually use in the network will be subclasses of the Node class. Each subclass has a special function.\n",
    "\n",
    "The first subclass we will create is Input. Inputs do not have any inbound_nodes so their values are passed in directly to the forward function.\n",
    "\n",
    "(I'll talk about the backward() method later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    '''\n",
    "    Subclass of Node. \n",
    "    Input has no inbound nodes\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        Node.__init__(self) #no inbound nodes\n",
    "\n",
    "    def forward(self, value=None):\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "    \n",
    "    def backward(self):\n",
    "        self.gradients = {self:0}\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] += grad_cost*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create an Add node which will sum it's inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    def __init__(self, *inputs):\n",
    "        Node.__init__(self, inputs)\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = sum([n.value for n in self.inbound_nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test what we have so far. We will create Input nodes and sum them together with the Add node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 + 5 + 4 = 19\n"
     ]
    }
   ],
   "source": [
    "x, y, z = Input(), Input(), Input()\n",
    "\n",
    "f = Add(x,y,z)\n",
    "\n",
    "feed_dict = {x:10, y:5, z:4}\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "output = forward_pass(f,sorted_nodes)\n",
    "\n",
    "print(\"{} + {} + {} = {}\".format(feed_dict[x], feed_dict[y], feed_dict[z], output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Seems to be working :)\n",
    "\n",
    "---\n",
    "Let's try a Mul node next that multiplies it's inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul(Node):\n",
    "    def __init__(self, *inputs):\n",
    "        Node.__init__(self, inputs)\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = 1\n",
    "        for n in self.inbound_nodes:\n",
    "            self.value *= n.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 x 5 x 4 = 200\n"
     ]
    }
   ],
   "source": [
    "x, y, z = Input(), Input(), Input()\n",
    "\n",
    "f = Mul(x,y,z)\n",
    "\n",
    "feed_dict = {x:10, y:5, z:4}\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "output = forward_pass(f,sorted_nodes)\n",
    "\n",
    "print(\"{} x {} x {} = {}\".format(feed_dict[x], feed_dict[y], feed_dict[z], output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! That works too!\n",
    "\n",
    "---\n",
    "So now let's get to the good stuff.\n",
    "\n",
    "A neural network is more or less a glorified function approximator. It approximates a function mapping it's inputs to it's outputs. The parameters which dictate the behaviour of this function are called weights. Each input value is simply multiplied by a weight and the sum of those products is added to a bias. This value can then be sent on as inputs to other nodes (with their own weights) or they can be sent to activation functions (which we'll soon cover) first. So all these values are controlled by weights and biases. \n",
    "\n",
    "Therefore, it's necessary to create a Linear node that takes in inputs, weights, and biases (constants) and will calculate a linear combination of it's inputs given by xW+b where x = inputs, W = weights, b = bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, inputs, weights, bias):\n",
    "        Node.__init__(self, [inputs, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        inputs = self.inbound_nodes[0].value\n",
    "        weights = self.inbound_nodes[1].value\n",
    "        bias = self.inbound_nodes[2].value\n",
    "\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "   \n",
    "    def backward(self):\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            #gradient wrt inputs\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "\n",
    "            #gradient wrt W\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "\n",
    "            #gradient wrt b\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.7\n"
     ]
    }
   ],
   "source": [
    "inputs, weights, bias = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(inputs, weights, bias)\n",
    "\n",
    "feed_dict = {\n",
    "    inputs: [6, 14, 3],\n",
    "    weights: [0.5, 0.25, 1.4],\n",
    "    bias: 2\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(output) # should be 12.7 with this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!\n",
    "\n",
    "---\n",
    "As I mentioned, the output of linear functions can be passed to other linear functions. But they can also be passed to activation functions. Activation functions serve to 1. Allow the network to estimate non-linear functions and 2. reduce the range of possible values that get propogated through the network. \n",
    "\n",
    "A popular activation function is the sigmoid function. Basically, it squishes any value to be between 0 and 1.\n",
    "Check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "    \n",
    "        self.value = 8\n",
    "    def _sigmoid(self, x):\n",
    "        x = 1./(1+np.exp(-x))\n",
    "        return x\n",
    "\n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(x)\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            \"\"\"\n",
    "            TODO: Your code goes here!\n",
    "\n",
    "            Set the gradients property to the gradients with respect to each input.\n",
    "\n",
    "            NOTE: See the Linear node and MSE node for examples.\n",
    "            \"\"\"\n",
    "\n",
    "            thisgrad = self.value*(1-self.value)\n",
    "            self.gradients[self.inbound_nodes[0]] += grad_cost*thisgrad\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.23394576e-04   9.82013790e-01]\n",
      " [  1.23394576e-04   9.82013790e-01]]\n"
     ]
    }
   ],
   "source": [
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "g = Sigmoid(f)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(g, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[  1.23394576e-04   9.82013790e-01]\n",
    " [  1.23394576e-04   9.82013790e-01]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have created a network that can approximate functions through a combination of linear and sigmoid nodes. \n",
    "\n",
    "That's great! But what if the network's approximation is incorrect. We need a way to improve the network's accuracy. This is done by adjusting the weights and biases, since those are the parmeters that control the function. So... how do we know how to adjust the weights?\n",
    "\n",
    "First, we need to know just how accurate the network is. So se need what's called a loss function or cost function.\n",
    "This is basically the error of the network - how far away the network's output is from the actual value. \n",
    "\n",
    "So, assuming we have the actual value and we have the networks output, we can calculate the error. There are many loss functions for calculating errors. Below I implement the mean squared error, which sums the squared differences of the output nodes and the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y,a])\n",
    "\n",
    "    def forward(self):\n",
    "        self.y = self.inbound_nodes[0].value.reshape(-1,1)\n",
    "        self.a = self.inbound_nodes[1].value.reshape(-1,1)\n",
    "\n",
    "        self.value = np.mean(np.square(self.y - self.a))\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        self.m = self.y.shape[0]\n",
    "        self.gradients[self.inbound_nodes[0]] = 2*(self.y-self.a)/self.m\n",
    "        self.gradients[self.inbound_nodes[1]] = -2*(self.y-self.a)/self.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4166666667\n"
     ]
    }
   ],
   "source": [
    "y, a = Input(), Input()\n",
    "cost = MSE(y, a)\n",
    "\n",
    "y_ = np.array([1, 2, 3])\n",
    "a_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y: y_, a: a_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "forward_pass(cost,graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "23.4166666667\n",
    "\"\"\"\n",
    "print(cost.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to talk about that backward() method. Now that we know how accurate (or inaccurate) the network's output is, we can adjust the weights accordingly. \n",
    "\n",
    "We therefore need to calculate the derivative of the loss function with respect to each weight. That's what the backward method in each node subclass is doing. it calculates the gradient of the cost function with respect to each it's input nodes based on the gradient with respect to it's output nodes using the chain rule. \n",
    "\n",
    "So far we have not actually called the backward method, so let's create a new method which does a backward pass (from outputs to inputs) each time it does a forward pass (from inputs to outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(graph):\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
      "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
      "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
      "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n"
     ]
    }
   ],
   "source": [
    "X, W, b = Input(), Input(), Input()\n",
    "y = Input()\n",
    "f = Linear(X, W, b)\n",
    "a = Sigmoid(f)\n",
    "cost = MSE(y, a)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2.], [3.]])\n",
    "b_ = np.array([-3.])\n",
    "y_ = np.array([1, 2])\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W: W_,\n",
    "    b: b_,\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_and_backward(graph)\n",
    "# return the gradients for each Input\n",
    "gradients = [t.gradients[t] for t in [X, y, W, b]]\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
    "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
    "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
    "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n",
    "\"\"\"\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The backward pass calculates the gradients.\n",
    "\n",
    "After we have done a backward pass, we have the gradients of the cost with respect to each weight and bias so now we can adjust the wieghts and biases.\n",
    "\n",
    "The gradient points in the direction of increasing slope. But our goal is to increase the accuracy which means minimize the loss. Therefore we have to move toward the minimum of the function ie. in the direction of the decreasing slope. \n",
    "\n",
    "So we must subtract the gradient from the parameter to move toward the minimum. But that's just part of it. That gives us the direction in which to move but now how much we should move. If we use the raw gradient value, we run the risk of overshooting and actually diverging from the minimum. So we create a hyperparameter called the learning rate. This is a small number by which we multiply the gradient before we subtract it from the value. \n",
    "\n",
    "This process is called Gradient Descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each gradient descent step updates the parameters (weights and biases) by moving it in the direction of decreasing gradient. Pushing the function closer to the minimum. To get better and better accuracy. We need to perform many gradient descent steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    for t in trainables:\n",
    "        t.value = t.value - learning_rate*t.gradients[t]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We have a perfectly functional basic Neural Network Library!\n",
    "\n",
    "Let's try training it on an actual datset of Boston House Prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 126.109\n",
      "Epoch: 2, Loss: 37.340\n",
      "Epoch: 3, Loss: 34.227\n",
      "Epoch: 4, Loss: 24.752\n",
      "Epoch: 5, Loss: 23.137\n",
      "Epoch: 6, Loss: 17.681\n",
      "Epoch: 7, Loss: 18.520\n",
      "Epoch: 8, Loss: 22.625\n",
      "Epoch: 9, Loss: 18.849\n",
      "Epoch: 10, Loss: 21.366\n",
      "Epoch: 11, Loss: 20.649\n",
      "Epoch: 12, Loss: 16.841\n",
      "Epoch: 13, Loss: 15.670\n",
      "Epoch: 14, Loss: 17.152\n",
      "Epoch: 15, Loss: 14.831\n",
      "Epoch: 16, Loss: 13.704\n",
      "Epoch: 17, Loss: 12.599\n",
      "Epoch: 18, Loss: 13.067\n",
      "Epoch: 19, Loss: 12.907\n",
      "Epoch: 20, Loss: 11.093\n",
      "Epoch: 21, Loss: 14.000\n",
      "Epoch: 22, Loss: 14.141\n",
      "Epoch: 23, Loss: 11.496\n",
      "Epoch: 24, Loss: 10.224\n",
      "Epoch: 25, Loss: 7.794\n",
      "Epoch: 26, Loss: 13.963\n",
      "Epoch: 27, Loss: 11.967\n",
      "Epoch: 28, Loss: 11.711\n",
      "Epoch: 29, Loss: 10.122\n",
      "Epoch: 30, Loss: 11.672\n",
      "Epoch: 31, Loss: 12.180\n",
      "Epoch: 32, Loss: 11.055\n",
      "Epoch: 33, Loss: 10.859\n",
      "Epoch: 34, Loss: 13.473\n",
      "Epoch: 35, Loss: 10.474\n",
      "Epoch: 36, Loss: 8.741\n",
      "Epoch: 37, Loss: 8.704\n",
      "Epoch: 38, Loss: 10.989\n",
      "Epoch: 39, Loss: 11.094\n",
      "Epoch: 40, Loss: 11.318\n",
      "Epoch: 41, Loss: 7.342\n",
      "Epoch: 42, Loss: 10.388\n",
      "Epoch: 43, Loss: 7.127\n",
      "Epoch: 44, Loss: 8.382\n",
      "Epoch: 45, Loss: 9.671\n",
      "Epoch: 46, Loss: 10.717\n",
      "Epoch: 47, Loss: 9.097\n",
      "Epoch: 48, Loss: 9.541\n",
      "Epoch: 49, Loss: 9.372\n",
      "Epoch: 50, Loss: 8.269\n",
      "Epoch: 51, Loss: 7.659\n",
      "Epoch: 52, Loss: 9.612\n",
      "Epoch: 53, Loss: 8.750\n",
      "Epoch: 54, Loss: 10.071\n",
      "Epoch: 55, Loss: 8.114\n",
      "Epoch: 56, Loss: 9.860\n",
      "Epoch: 57, Loss: 8.121\n",
      "Epoch: 58, Loss: 9.458\n",
      "Epoch: 59, Loss: 7.549\n",
      "Epoch: 60, Loss: 7.948\n",
      "Epoch: 61, Loss: 8.320\n",
      "Epoch: 62, Loss: 7.188\n",
      "Epoch: 63, Loss: 7.390\n",
      "Epoch: 64, Loss: 7.981\n",
      "Epoch: 65, Loss: 7.803\n",
      "Epoch: 66, Loss: 9.735\n",
      "Epoch: 67, Loss: 7.594\n",
      "Epoch: 68, Loss: 7.967\n",
      "Epoch: 69, Loss: 7.555\n",
      "Epoch: 70, Loss: 8.237\n",
      "Epoch: 71, Loss: 7.998\n",
      "Epoch: 72, Loss: 9.562\n",
      "Epoch: 73, Loss: 6.613\n",
      "Epoch: 74, Loss: 8.839\n",
      "Epoch: 75, Loss: 6.925\n",
      "Epoch: 76, Loss: 7.041\n",
      "Epoch: 77, Loss: 8.928\n",
      "Epoch: 78, Loss: 7.026\n",
      "Epoch: 79, Loss: 6.661\n",
      "Epoch: 80, Loss: 8.627\n",
      "Epoch: 81, Loss: 5.917\n",
      "Epoch: 82, Loss: 6.821\n",
      "Epoch: 83, Loss: 5.703\n",
      "Epoch: 84, Loss: 6.444\n",
      "Epoch: 85, Loss: 8.491\n",
      "Epoch: 86, Loss: 7.532\n",
      "Epoch: 87, Loss: 6.112\n",
      "Epoch: 88, Loss: 8.808\n",
      "Epoch: 89, Loss: 6.749\n",
      "Epoch: 90, Loss: 7.931\n",
      "Epoch: 91, Loss: 7.870\n",
      "Epoch: 92, Loss: 6.846\n",
      "Epoch: 93, Loss: 6.232\n",
      "Epoch: 94, Loss: 6.401\n",
      "Epoch: 95, Loss: 7.156\n",
      "Epoch: 96, Loss: 7.745\n",
      "Epoch: 97, Loss: 7.138\n",
      "Epoch: 98, Loss: 5.718\n",
      "Epoch: 99, Loss: 6.670\n",
      "Epoch: 100, Loss: 6.167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 100\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        sgd_update(trainables)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
